{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd29463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습 관련 기술들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화, 초깃값, 하이퍼파라미터, 오버피팅 대응책(가중치 감소, 드롭아웃), 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 매개변수 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것  \n",
    "이는 곧 최적 매개변수를 찾는 문제이며, 이러한 문제를 푸는 것을 `최적화(optimization)`라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 모험가 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "광대하고 복잡한 지형을 지도도 없이 눈을 가린채로 '깊은 곳'을 찾지 않으면 안되는 상황에서  \n",
    "중요한 단서가 되는 것이 땅의 `기울기`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 확률적 경사 하강법(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수식으로 쓰면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 SGD의 단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 함수의 최솟값을 구하는 문제를 생각해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x,y) = \\frac{1}{20}x^2 + y^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수의 그래프와 등고선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"648\" alt=\"fig 6-1\" src=\"https://user-images.githubusercontent.com/78716519/194982136-b6e76067-1ed4-409a-95b1-79f8f1c98424.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수의 기울기를 그려보면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"551\" alt=\"fig 6-2\" src=\"https://user-images.githubusercontent.com/78716519/194982284-f449d976-a6ca-4c88-87a3-511961e9734a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 기울기는 y축 방향은 크고, x축 방향은 작음  \n",
    "즉 y축 방향은 가파른데, x축 방향은 완만해서  \n",
    "최솟값이 되는 장소는 $(x,y) = (0,0)$이지만  \n",
    "위의 그림이 보여주는 기울기 대부분은 $(0,0)$ 방향을 가리키지 않는다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 함수에 SGD를 적용해보면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"563\" alt=\"fig 6-3\" src=\"https://user-images.githubusercontent.com/78716519/194982516-2a938087-a225-441e-b9ef-a2862bf19aa9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비효율적인 움직임 → SGD의 이러한 단점을 개선해주는 Momentum, AdaGrad, Adam 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 모멘텀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`모멘텀(momentum)`: '운동량'을 뜻하는 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{v} \\leftarrow \\alpha \\mathbf{v} - \\eta \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} $$\n",
    "$$ \\mathbf{W} \\leftarrow \\mathbf{W} + \\mathbf{v} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}$: 갱신할 가중치 매개변수  \n",
    "$\\frac{\\partial{L}}{\\partial{\\mathbf{W}}}$: $\\mathbf{W}$에 대한 손실함수의 기울기  \n",
    "$\\eta$: 학습률  \n",
    "$\\mathbf{v}$: 물리에서 말하는 속도(velocity)  \n",
    "$\\alpha \\mathbf{v}$항은 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할  \n",
    "$\\alpha$: 물리에서는 지면 마찰이나 공기 저항에 해당, $0.9$ 등의 값으로 설정함\n",
    "\n",
    "첫번째 식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타냄  \n",
    "모멘텀은 아래의 그림과 같이 공이 그릇의 바닥을 구르는 듯한 움직임을 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"629\" alt=\"fig 6-4\" src=\"https://user-images.githubusercontent.com/78716519/194983373-7cfbcf3d-4f39-4667-9af5-e437fdfb93e5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None # 초기화 때는 아무 값도 담지 않음\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val) # 매개변수와 같은 구조의 데이터를 딕셔너리 변수로 저장\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모멘텀을 사용해서 6.1.3의 함수의 최적화 문제를 풀어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"557\" alt=\"fig 6-5\" src=\"https://user-images.githubusercontent.com/78716519/194984403-7a73bfa6-4e80-464e-93d9-375501962fc1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모멘텀의 갱신 경로는 공이 그릇 바닥을 구르듯 움직임  \n",
    "SGD와 비교하면 '지그재그 정도'가 덜함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서는 학습률이 중요한데  \n",
    "이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 올바른 학습을 할 수 없음  \n",
    "\n",
    "학습률을 정하는 효과적 기술로  \n",
    "`학습률 감소(learning rate decay)`: 학습을 진행하면서 학습률을 점차 줄여가는 방법, 처음에는 크게 학습하다가 조금씩 작게 학습하는 것  \n",
    "\n",
    "학습률을 서서히 낮추는 가장 간단한 방법은 매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 것  \n",
    "이를 발전시킨 것이  \n",
    "`AdaGrad`: '각각의' 매개변수에 '맞춤형' 학습률 값을 만들어줌  \n",
    "\n",
    "수식으로 쓰면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h \\leftarrow h + \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} \\odot \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} $$  \n",
    "$$ \\mathbf{W} \\leftarrow \\mathbf{W} + \\eta \\frac{1}{\\sqrt{h}} \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}$: 갱신할 가중치 매개변수  \n",
    "$\\frac{\\partial{L}}{\\partial{\\mathbf{W}}}$: $\\mathbf{W}$에 대한 손실함수의 기울기  \n",
    "$\\eta$: 학습률  \n",
    "$h$: 기존 기울기 값을 제곱하여 계속 더해주는 변수 ($\\odot$기호는 행렬의 원소별 곱셈을 의미)  \n",
    "그리고 매개변수를 갱신할 때 $\\frac{1}{\\sqrt{h}}$을 곱해 학습률을 조정함  \n",
    "매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아진다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad는 과거의 기울기를 제곱하여 계속 더해감 → 학습을 진행할수록 갱신 강도가 약해짐  \n",
    "실제로 무한히 계속 학습한다면 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않게 됨  \n",
    "이 문제를 개선한 기법이  \n",
    "`RMSProp`: 과거의 모든 기울기를 균일하게 더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영함  \n",
    "이를 `지수이동평균(exponential moving average, EMA)`이라 하여, 과거 기울기의 반영 규모를 기하급수적으로 감소시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr / (np.sqrt(self.h[key]) + 1e-7) * grads[key] # 1e-7 이라는 작은 값을 더해줌으로써 분모가 0이 되지 않도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"558\" alt=\"fig 6-6\" src=\"https://user-images.githubusercontent.com/77653353/195875561-c5e9cbfe-669d-4098-b114-04023c3ced35.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6 Adam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
