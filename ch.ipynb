{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd29463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71613af",
   "metadata": {},
   "source": [
    "# 3. 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec52d75",
   "metadata": {},
   "source": [
    "## 3.1 퍼셉트론에서 신경망으로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146bf19",
   "metadata": {},
   "source": [
    "### 3.1.3 활성화 함수의 등장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71305d08",
   "metadata": {},
   "source": [
    "<code>활성화 함수(activation function)</code>: 입력 신호의 총합이 활성화를 일으키는지 정하는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75836bc",
   "metadata": {},
   "source": [
    "## 3.2 활성화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b349004",
   "metadata": {},
   "source": [
    "퍼셉트론에서는 활성화 함수로 계단 함수(step function)을 활용  \n",
    "신경망에서는 활성화 함수로 시그모이드 함수, 렐루 함수 등을 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe5502",
   "metadata": {},
   "source": [
    "### 3.2.1 시그모이드 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71bc68",
   "metadata": {},
   "source": [
    "<code>시그모이드 함수(sigmoid function)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cef1e8",
   "metadata": {},
   "source": [
    "$$ h(x) = \\frac{1}{1+e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59e85a",
   "metadata": {},
   "source": [
    "$e$는 자연상수로 $2.7182...$의 값을 갖는 실수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3150727",
   "metadata": {},
   "source": [
    "### 3.2.4 시그모이드 함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd918f",
   "metadata": {},
   "source": [
    "### 3.2.6 비선형 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faae795",
   "metadata": {},
   "source": [
    "- 선형 함수: 무언가 입력했을 때 출력이 입력의 상수배만큼 변하는 함수, $f(x) = ax + b$, $a$와 $b$는 상수  \n",
    "- 비선형 함수: 선형이 아닌 함수, 직선 1개로는 그릴 수 없는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf0236",
   "metadata": {},
   "source": [
    "신경망에서는 활성화 함수로 비선형 함수를 사용해야 함  \n",
    "선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a5f44",
   "metadata": {},
   "source": [
    "선형 함수인 $h(x) = cx$를 활성화 함수로 사용한 3층 네트워크를 식으로 나타내보면  \n",
    "$y(x) = h(h(h(x)))$가 되고, 이 계산은 $y(x) = c*c*c*x$처럼 세 번의 곱셈을 수행하지만  \n",
    "실은 $y(x) = ax$와 똑같은 식임, $a = c^3$이라고 하면 끝임\n",
    "즉, 은닉층이 없는 네트워크로 표현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e15724",
   "metadata": {},
   "source": [
    "### 3.2.7 ReLU 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac034ff",
   "metadata": {},
   "source": [
    "<code>ReLU 함수(Rectified Linear Unit function)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bc49e",
   "metadata": {},
   "source": [
    "$$ h(x) = \\begin{cases} x, & (x>0) \\\\ 0, & (x\\le0) \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3a9f7",
   "metadata": {},
   "source": [
    "## 3.5 출력층 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc3a6a",
   "metadata": {},
   "source": [
    "- <code>분류(classification)</code>: 데이터가 어느 클래스(class)에 속하느냐는 문제  \n",
    "  \\- 시그모이드(sigmoid) 함수: 이진 클래스 분류  \n",
    "  \\- 소프트맥스(softmax) 함수: 다중 클래스 분류  \n",
    "- <code>회귀(regression)</code>: 입력 데이터에서 (연속적인) 수치를 예측하는 문제  \n",
    "  \\- 항등(identity) 함수: 입력 = 출력 (입력을 그대로 출력)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c858c",
   "metadata": {},
   "source": [
    "### 3.5.1 항등 함수와 소프트맥스 함수 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442bd0c3",
   "metadata": {},
   "source": [
    "<code>소프트맥스(Softmax)</code> 함수 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d74684",
   "metadata": {},
   "source": [
    "$$ y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^{n} \\exp(a_i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ba7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192922c",
   "metadata": {},
   "source": [
    "### 3.5.2 소프트맥스 함수 구현 시 주의점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c267c41",
   "metadata": {},
   "source": [
    "#### 오버플로 문제\n",
    "소프트맥스 함수는 지수 함수를 사용하는데, 아주 큰 값을 출력하기 쉬움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48cae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 0, exp(x) = 1.0\n",
      "x = 50, exp(x) = 5.184705528587072e+21\n",
      "x = 100, exp(x) = 2.6881171418161356e+43\n",
      "x = 150, exp(x) = 1.3937095806663797e+65\n",
      "x = 200, exp(x) = 7.225973768125749e+86\n",
      "x = 250, exp(x) = 3.7464546145026734e+108\n",
      "x = 300, exp(x) = 1.9424263952412558e+130\n",
      "x = 350, exp(x) = 1.0070908870280797e+152\n",
      "x = 400, exp(x) = 5.221469689764144e+173\n",
      "x = 450, exp(x) = 2.7071782767869983e+195\n",
      "x = 500, exp(x) = 1.4035922178528375e+217\n",
      "x = 550, exp(x) = 7.277212331783397e+238\n",
      "x = 600, exp(x) = 3.7730203009299397e+260\n",
      "x = 650, exp(x) = 1.956199921370272e+282\n",
      "x = 700, exp(x) = 1.0142320547350045e+304\n",
      "x = 750, exp(x) = inf\n",
      "x = 800, exp(x) = inf\n",
      "x = 850, exp(x) = inf\n",
      "x = 900, exp(x) = inf\n",
      "x = 950, exp(x) = inf\n",
      "x = 1000, exp(x) = inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22776\\2287531336.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  print(f'x = {x}, exp(x) = {np.exp(x)}')\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 1001, 50):\n",
    "    print(f'x = {x}, exp(x) = {np.exp(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2589610",
   "metadata": {},
   "source": [
    "무한대(inf)와 같은 큰 값끼리 나눗셈을 하면 결과 수치가 불안정해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e73ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22776\\832863605.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(a) / np.sum(np.exp(a))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22776\\832863605.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  np.exp(a) / np.sum(np.exp(a))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.array([1010, 1000, 990])\n",
    "np.exp(a) / np.sum(np.exp(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae1b95",
   "metadata": {},
   "source": [
    "아무런 조치 없이 그냥 계산하면 nan이 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f4b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, -10, -20])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = np.max(a)\n",
    "a - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d07f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = np.exp(a-c) / np.sum(np.exp(a-c))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf9f7b",
   "metadata": {},
   "source": [
    "입력 신호 중 최댓값을 빼주면 올바르게 계산 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # 오버플로 대책\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae0a2a",
   "metadata": {},
   "source": [
    "### 3.5.3 소프트맥스 함수의 특징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "y = softmax(a)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1136c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a207cf",
   "metadata": {},
   "source": [
    "소프트맥스 함수 출력의 총합은 1 → <code>확률</code>로 해석 가능  \n",
    "위에서는 y[0]의 확률은 1.8%, y[1]의 확률은 24.5%, y[2]의 확률은 73.7%라고 해석 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5fc038",
   "metadata": {},
   "source": [
    "기계학습의 문제 풀이는 <code>학습(training)</code>과 <code>추론(inference)</code>의 두 단계를 거쳐 이뤄짐  \n",
    "학습 단계에서 모델을 학습하고, 추론 단계에서 앞서 학습한 모델로 미지의 데이터에 대해서 추론(분류)을 수행함  \n",
    "학습 시, 출력층에서 소프트맥스 함수를 사용하지만  \n",
    "추론 시, (현업에서는) 지수 함수 계산에 드는 자원 낭비를 줄이고자 출력층의 소프트맥스 함수는 생략하는 것이 일반적임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d9ff3",
   "metadata": {},
   "source": [
    "### 3.5.4 출력층의 뉴런 수 정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf37639",
   "metadata": {},
   "source": [
    "출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야함  \n",
    "분류에서는 분류하고 싶은 클래스 수로 정하는 것이 일반적임  \n",
    "ex. 이미지를 숫자 0부터 9 중 하나로 분류하는 문제라면 출력층의 뉴런을 10개로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8d0c62",
   "metadata": {},
   "source": [
    "## 3.6 손글씨 숫자 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65320f25",
   "metadata": {},
   "source": [
    "### 3.6.1 MNIST 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bfae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1eb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af396de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee154fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "img = img.reshape(28, 28)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97453fda",
   "metadata": {},
   "source": [
    "### 3.6.2 신경망의 추론 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444dba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def init_network():\n",
    "    with open(\"dataset/sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y)\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233519e",
   "metadata": {},
   "source": [
    "### 3.6.3 배치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>학습</code>: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 평균 제곱 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>평균 제곱 오차(mean squared error, MSE)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = \\frac{1}{n} \\sum_{k}(y_k - t_k)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차를 제곱하고 모두 합한 후 평균  \n",
    "\n",
    "$y_k$: 신경망의 출력(신경망이 추정한 값)  \n",
    "$t_k$: 정답 레이블  \n",
    "$k$: 데이터의 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return np.sum((y-t)**2) / len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>교차 엔트로피 오차(cross entropy error, CEE)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = -\\sum_{k} t_{k} \\log{y_{k}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\log$는 밑이 $e$인 자연로그  \n",
    "\n",
    "$y_k$: 신경망의 출력  \n",
    "$t_k$: 정답 레이블  \n",
    "\n",
    "$t_k$는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0 (원핫인코딩)  \n",
    "→ $t_k=0$일 때는 모두 무시 가능하고, $t_k=1$일 때(정답일 때)만 자연로그 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 정답 레이블은 '2'가 정답이라 하고 이때의 신경망 출력 $y_k$가 $0.6$이라면 교차 엔트로피 오차는 $-\\log0.6 = 0.51$  \n",
    "같은 조건에서 신경망 출력 $y_k$가 $0.1$이라면 교차 엔트로피 오차는 $-\\log0.1 = 2.3$  \n",
    "(상대적으로 제대로 예측한 경우(신경망 출력이 $0.6$인 경우) loss가 작고  \n",
    " 상대적으로 잘못 예측한 경우(신경망 출력이 $0.1$인 경우) loss가 큼)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"473\" src=\"https://user-images.githubusercontent.com/77653353/192322965-5d57ab8b-a5b1-4b2f-a79c-c9a0cae2b55d.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 작은 값인 delta를 더하여 절대 0이 되지 않도록, 즉 마이너스 무한대가 발생하지 않도록 한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터 하나가 아닌 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 생각해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터 모두에 대한 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = - \\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log y_{nk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 $N$개라면 $t_{nk}$는 $n$번째 데이터의 $k$차원 째의 값을 의미  \n",
    "\n",
    "$y_{nk}$: 신경망의 출력  \n",
    "$t_{nk}$: 정답 레이블  \n",
    "\n",
    "수식이 좀 복잡해 보이지만 데이터 하나에 대한 손실함수를 단순히 $N$개의 데이터로 확장했을 뿐임  \n",
    "마지막에 $N$으로 나누어 정규화하여 '평균 손실 함수'를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수백만, 수천만개의 데이터를 일일이 계산하기에는 쉽지 않기 때문에  \n",
    "훈련 데이터로부터 일부만 골라 학습을 수행하는데 이 일부를 <code>미니배치(mini-batch)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 레이블이 원-핫 인코딩인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "(2, 10)\n",
      "1.7583023994178049\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0, 0, 0.02, 0.03, 0,    0, 0, 0.1, 0.05, 0.8],\\\n",
    "              [0, 0, 0   , 0,    0.01, 0, 0, 0,   0.99, 0  ]])\n",
    "t = np.array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\\\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])\n",
    "\n",
    "delta = 1e-7\n",
    "batch_size = y.shape[0]\n",
    "\n",
    "print(y.shape)\n",
    "print(t.shape)\n",
    "print(-np.sum(t * np.log(y + delta)) / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 레이블이 원-핫 인코딩이 아니라 '$2$'나 '$7$' 등의 숫자 레이블로 주어지는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "(2,)\n",
      "[[0.   0.   0.02 0.03 0.   0.   0.   0.1  0.05 0.8 ]\n",
      " [0.   0.   0.   0.   0.01 0.   0.   0.   0.99 0.  ]]\n",
      "[0 1]\n",
      "[3 8]\n",
      "[0.03 0.99]\n",
      "1.7583023994178049\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0, 0, 0.02, 0.03, 0,    0, 0, 0.1, 0.05, 0.8],\\\n",
    "              [0, 0, 0   , 0,    0.01, 0, 0, 0,   0.99, 0  ]])\n",
    "t = np.array([3, 8])\n",
    "\n",
    "delta = 1e-7\n",
    "batch_size = y.shape[0]\n",
    "\n",
    "\n",
    "print(y.shape)\n",
    "print(t.shape)\n",
    "print(y)\n",
    "print(np.arange(batch_size))\n",
    "print(t)\n",
    "print(y[np.arange(batch_size), t]) # [ y[0,3] y[1,8] ]\n",
    "\n",
    "y = y + delta\n",
    "print(-np.sum(np.log(y[np.arange(batch_size), t])) / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 레이블이 숫자 레이블로 주어지는 경우, np.log(y[np.arange(batch_size), t])  \n",
    "np.arange(batch_size)는 0부터 batch_size-1까지 배열을 생성함  \n",
    "ex. batch_size=5라면,  \n",
    "np.arange(batch_size)는 [0,1,2,3,4]라는 넘파이 배열 생성  \n",
    "t에는 레이블이 [2,7,0,9,4]와 같이 저장되어 있으므로  \n",
    "y[np.arange(batch_size), t]는 y[[0,1,2,3,4], [[2,7,0,9,4]]] → [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [1 2 3]\n",
      "a.shape = (3,)\n",
      "a.shape[0] = 3\n",
      "\n",
      "b = [[1 2 3]]\n",
      "b.shape = (1, 3)\n",
      "b.shape[0] = 1\n"
     ]
    }
   ],
   "source": [
    "# 위에서 reshape을 하는 이유?\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "print(f'a = {a}')\n",
    "print(f'a.shape = {a.shape}')\n",
    "print(f'a.shape[0] = {a.shape[0]}')\n",
    "print()\n",
    "\n",
    "b = a.reshape(1, a.size)\n",
    "print(f'b = {b}')\n",
    "print(f'b.shape = {b.shape}')\n",
    "print(f'b.shape[0] = {b.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 하나인 경우(batch_size가 1인 경우)  \n",
    "shape가 (3,) 처럼 나오기 때문에  \n",
    "a.shape[0]가 1이 아닌 3으로 잘못된 batch_size가 입력됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 왜 손실 함수를 설정하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 궁극적인 목적은 높은 '정확도'를 끌어내는 매개변수 값을 찾는 것!  \n",
    "그렇다면 왜 '정확도'라는 지표를 두고 손실 함수의 값을 거쳐갈까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서의 <code>미분</code>의 역할에 주목!  \n",
    "가령 가상의 신경망이 있고, 그 신경망의 어느 한 가중치 매개변수에 주목한다고 해보자  \n",
    "이때 그 가중치 매개변수의 손실 함수의 미분이란 '가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나'라는 의미  \n",
    "만약 이 미분 값이 음수면, 그 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있고  \n",
    "만약 이 미분 값이 양수면, 그 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있음  \n",
    "만약 이 미분 값이 0이면, 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 달라지지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계단 함수의 미분은 대부분의 장소(0 이외의 곳)에서 0 → 계단 함수를 활성화 함수로 이용하면 손실함수를 지표로 삼는 게 아무 의미 없어짐  \n",
    "매개변수의 작은 변화가 주는 파장을 계단 함수가 말살하여 손실 함수의 값에는 아무런 변화가 나타나지 않기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>미분</code>: 특정 순간의 변화량  \n",
    "$x$의 작은 변화가 함수$f(x)$를 얼마나 변화시키느냐를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{df(x)}{dx} = \\lim_{h→0}\\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 참고하여 함수의 미분을 구하는 계산을 구현해보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 방식은 두 가지 문제가 있는데\n",
    "- 반올림 오차 문제 → 미세한 값 $h$를 $10^{-4}$정도로 조절\n",
    "- 진정한 미분은 $x$위치의 함수의 기울기(접선)이지만 $h$를 무한히 0으로 좁히는 것 불가 → 중심 차분 또는 중앙 차분 사용($(x+h)$와 $(x-h)$일 때의 함수 $f$의 차분 계산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>수치 미분(numerical differentiation)</code>: 아주 작은 차분(임의 두 점에서의 함수 값들의 차이)으로 미분을 구하는 것  \n",
    "<code>해석적 미분(analytic differentiation)</code>: 수식을 전개해 미분을 구하는 것 ex.$y = x^2$의 미분은 $\\frac{dy}{dx} = 2x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x_0, x_1) = x_0^2 + x_1^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 함수가 있다고 해보자  \n",
    "인수들의 제곱 합을 계산하는 단순한 식이지만, 이번엔 변수가 2개인 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"473\" alt=\"fig 4-8\" src=\"https://user-images.githubusercontent.com/77653353/193848270-9e48b89b-d329-4663-9797-3095f4e597f9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>편미분</code>: 변수가 여럿인 함수에 대한 미분, $\\frac{\\partial{f}}{\\partial{x_0}}$이나 $\\frac{\\partial{f}}{\\partial{x_1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서는 $x_0$와 $x_1$의 편미분을 변수별로 따로 계산했음  \n",
    "그렇다면 $x_0$와 $x_1$의 편미분을 동시에 계산하려면?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_0=3$, $x_1=4$일 때 $(x_0, x_1)$ 양쪽의 편미분을 묶어서 $(\\frac{\\partial{f}}{\\partial{x_0}}, \\frac{\\partial{f}}{\\partial{x_1}})$처럼  \n",
    "모든 변수의 편미분을 벡터로 정리한 것을 <code>기울기(gradient)</code>라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):               # [3, 4]\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)                 # [0, 0]\n",
    "\n",
    "    for idx in range(x.size):               # idx=0, x.size=2\n",
    "        tmp_val = x[idx]                    # tmp_val = 3\n",
    "\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h                # x[0] = 3.0001\n",
    "        fxh1 = f(x)                         # fxh1 = 25.00060001\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h                # x[0] = 2.9999\n",
    "        fxh2 = f(x)                         # fxh2 = 24.99940001\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)   # grad[0] = 0.0012 / 0.0002 = 6\n",
    "        x[idx] = tmp_val # 값 복원          # x[0] = 3\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기는 함수의 '가장 낮은 장소(최솟값)'를 가리키는 것  \n",
    "기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 줄이는 방향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법(경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 목표는 학습 단계에서 최적의 매개변수를 찾는 것이고, 이는 손실 함수가 최솟값이 될 때의 매개변수 값  \n",
    "하지만 광대한 공간 속에서 어느 곳이 최솟값인지 알아내기 쉽지 않음  \n",
    "이러한 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수가 최솟값, 극솟값, 안장점이 되는 장소에서는 기울기가 0  \n",
    "- 극솟값: 국소적인 최솟값  \n",
    "- 안장점(saddle point): 어느 방향에서 보면 극댓값이고, 다른 방향에서 보면 극솟값이 되는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울어진 방향이 꼭 최솟값을 가리키는 것은 아니지만, 그 방향으로 가야 함수의 값을 줄일 수 있음  \n",
    "그래서 최솟값이 되는 장소를 찾는 문제에서는 기울기 정보를 단서로 나아갈 방향을 정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>경사법(gradient method)</code>, <code>경사 하강법(gradient descent method)</code>:  \n",
    "현 위치에서 기울어진 방향으로 일정 거리만큼 이동하고, 이동한 곳에서도 기울기를 구하고, 또 기울어진 방향으로 나아가서 함수의 값을 점차 줄이는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수식으로 나타내면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x_0 = x_0 - \\eta \\frac{\\partial{f}}{\\partial{x_0}} $$  \n",
    "$$ x_1 = x_1 - \\eta \\frac{\\partial{f}}{\\partial{x_1}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>학습률(learning rate)</code>: 매개변수 값을 얼마나 갱신하느냐를 정하는 것, 여기서는 $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 하강법 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):     # f: 최적화하려는 함수\n",
    "    x = init_x                                              # init_x: 초깃값\n",
    "                                                            # lr: 학습률\n",
    "    for i in range(step_num):                               # step_num: 경사법에 따른 반복 횟수\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>하이퍼파라미터(hyper parameter)</code>: 사람이 직접 설정해야 하는 매개변수 ex.학습률  \n",
    "<code>파라미터(parameter)</code>: 훈련 데이터와 학습 알고리즘에 의해서 '자동'으로 획득되는 매개변수 ex.가중치, 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전제  \n",
    "  신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 함. 신경망 학습은 다음과 같이 4단계로 수행함  \n",
    "  \n",
    "- 1단계 - 미니배치  \n",
    "  훈련 데이터 중 일부를 무작위로 가져옴. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것을 목표로 함  \n",
    "\n",
    "- 2단계 - 기울기 산출  \n",
    "  미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시  \n",
    "\n",
    "- 3단계 - 매개변수 갱신  \n",
    "  가중치 매개변수를 기울기 방향으로 아주 조금 갱신함  \n",
    "\n",
    "- 4단계 - 반복  \n",
    "  1~3단계를 반복함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>확률적 경사 하강법(Stochastic Gradient Descent, SGD)</code>: 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient 메서드는 수치 미분 방식으로 각 매개변수의 손실 함수에 대한 기울기를 계산  \n",
    "gradient 메서드는 오차역전파법을 사용하여 기울기를 계산 (다음 장에서 진행)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 시험 데이터로 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드에서 1에포크별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록함  \n",
    "<code>에포크(epoch)</code>: 학습에서 훈련 데이터를 모두 소진했을 때의 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 오차역전파법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 장에서는 신경망의 가중치 매개변수의 기울기는 수치 미분을 사용해서 구함  \n",
    "수치 미분은 시간이 오래 걸린다는 단점  \n",
    "이번 장에서는 가중치 매개변수의 기울기를 효율적으로 계산하는 오차역전파법을 다룸  \n",
    "오차역전파법을 이해하는 방법 두 가지: 수식, 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>계산 그래프(computational graph)</code>: 계산 과정을 그래프로 나타낸 것, 노드(node)와 에지(edge)로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 계산 그래프로 풀다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex. 현빈 군은 슈퍼에서 1개에 100원인 사과를 2개 샀습니다. 이때 지불 금액을 구하세요. 단, 소비세가 10% 부과됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"601\" alt=\"fig 5-2\" src=\"https://user-images.githubusercontent.com/77653353/194425362-70f6152d-7220-4027-8207-2ae909133086.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>순전파(forward propagation)</code>: 계산을 왼쪽에서 오른쪽으로 진행하는 단계  \n",
    "<code>역전파(backward propagation)</code>: 계산을 오른쪽에서 왼쪽으로 반대로 진행하는 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 국소적 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산 그래프는 <code>국소적 계산</code>에 집중함  \n",
    "전체 계산이 아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 국소적 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"643\" alt=\"fig 5-4\" src=\"https://user-images.githubusercontent.com/77653353/194426040-1b161e9b-348a-4ae7-8bc4-77543d484943.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서는 여러 식품을 구입(복잡한 계산)을 거쳐 총 금액이 4,000원이 되었는데  \n",
    "사과와 다른 물품 값을 더하는 계산(4,000 + 200 = 4,200)은  \n",
    "4,000이라는 숫자가 어떻게 계산되었느냐와는 상관없이, 단지 두 숫자를 더하면 된다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 왜 계산 그래프로 푸는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산 그래프를 사용하는 가장 큰 이유는 역전파를 통해 <code>미분</code>을 효율적으로 계산할 수 있다는 점!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맨 위의 예시에서  \n",
    "만약 사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지를 알고 싶다고 해보자  \n",
    "이는 '사과 가격에 대한 지불 금액의 미분'을 구하는 문제에 해당됨  \n",
    "사과 값을 $x$, 지불 금액을 $L$이라 했을 때 $\\frac{\\partial{L}}{\\partial{x}}$를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"608\" alt=\"fig 5-5\" src=\"https://user-images.githubusercontent.com/77653353/194427222-dbe08b8b-4e0e-4d25-9e89-ea908a9fe499.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파는 오른쪽에서 왼쪽으로 '1 → 1.1 → 2.2' 순으로 미분 값을 전달함  \n",
    "사과가 1원 오르면 최종 금액은 2.2원 오른다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 연쇄법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "국소적 미분을 전달하는 원리는 <code>연쇄법칙(chain rule)</code>에 따른 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 계산 그래프의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"282\" alt=\"fig 5-6\" src=\"https://user-images.githubusercontent.com/77653353/194427676-a92a8aa2-b16b-474b-83b1-3e375243ab16.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파의 계산 절차는 신호 $E$에 노드의 국소적 미분 $\\frac{\\partial{y}}{\\partial{x}}$을 곱한 후 다음 노드로 전달하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 연쇄법칙이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연쇄법칙을 설명하려면 우선 합성 함수부터  \n",
    "<code>합성 함수</code>: 여러 함수로 구성된 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex. $z = (x+y)^2$이라는 식은 아래처럼 두 개의 식으로 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z = t^2 $$\n",
    "$$ t = x + y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다' 가 연쇄법칙의 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수식으로 쓰면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{z}}{\\partial{x}} = \\frac{\\partial{z}}{\\partial{t}} \\frac{\\partial{t}}{\\partial{x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{z}}{\\partial{t}} = 2t $$  \n",
    "$$ \\frac{\\partial{t}}{\\partial{x}} = 1 $$  \n",
    "$$ \\frac{\\partial{z}}{\\partial{x}} = 2t \\cdot 1 = 2(x+y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 연쇄법칙과 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 연쇄법칙을 계산 그래프로 나타내면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"466\" alt=\"fig 5-7\" src=\"https://user-images.githubusercontent.com/77653353/194429186-34492d6f-3168-4762-bde2-ff5ace04d458.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 덧셈 노드의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = x + y$ 라는 식이 있다면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{z}}{\\partial{x}} = 1 $$  \n",
    "$$ \\frac{\\partial{z}}{\\partial{y}} = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽이 순전파, 오른쪽이 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"651\" alt=\"fig 5-9\" src=\"https://user-images.githubusercontent.com/77653353/194429956-0449676d-80a9-4e1d-bca8-e19c1661038e.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 상류에서 전해진 미분이 $\\frac{\\partial{L}}{\\partial{z}}$이라고 한다면  \n",
    "덧셈 노드의 역전파는 <code>입력된 값을 그대로 다음 노드로</code> 보내게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 곱셈 노드의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = xy$ 라는 식이 있다면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{z}}{\\partial{x}} = y $$  \n",
    "$$ \\frac{\\partial{z}}{\\partial{y}} = x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽이 순전파, 오른쪽이 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"651\" alt=\"fig 5-12\" src=\"https://user-images.githubusercontent.com/77653353/194430669-ae8cb860-49a4-4916-8e17-c8b3e28ab4b4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 상류에서 전해진 미분이 $\\frac{\\partial{L}}{\\partial{z}}$이라고 한다면  \n",
    "곱곱 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 <code>서로 바꾼 값</code>을 곱해서 하류로 보내게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "덧셈의 역전파에서는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값은 필요하지 않지만  \n",
    "곱셈의 역전파에서는 순방향 입력 신호의 값이 필요하기에 곱셈 노드 구현 시 순전파의 입력 신호를 유지함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 단순한 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # x와 y를 바꾼다.\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLU 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = \\begin{cases}\n",
    "x, & (x>0) \\\\\n",
    "0, & (x \\le 0)\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{y}}{\\partial{x}} = \\begin{cases}\n",
    "1, & (x>0) \\\\\n",
    "0, & (x \\le 0)\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"638\" alt=\"fig 5-18\" src=\"https://user-images.githubusercontent.com/77653353/194433624-d83e7e9a-23a6-472a-ac2e-7c420f9d0fec.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 mask라는 인스턴스 변수는 True/False로 구성된 넘파이 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"644\" alt=\"fig 5-19\" src=\"https://user-images.githubusercontent.com/77653353/194434874-d6dfd4a8-5cf6-4176-acf0-e96d94ceb8e4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"669\" alt=\"fig 5-20\" src=\"https://user-images.githubusercontent.com/77653353/194436706-1af8b171-d15f-4430-9e06-9ad0db8d798f.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 계산 그래프의 중간 과정을 그룹화하여 아래처럼 단순한 sigmoid 노드 하나로 대체 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"305\" alt=\"fig 5-21\" src=\"https://user-images.githubusercontent.com/77653353/194436828-6c066b55-4ef0-4b19-819c-02a06ce11ce0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 $\\frac{\\partial{L}}{\\partial{y}} y^2 e^{-x}$는 아래처럼 정리하여 쓸 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{matrix}\n",
    "\\frac{\\partial{L}}{\\partial{y}} y^2 e^{-x} &=& \\frac{\\partial{L}}{\\partial{y}} \\frac{1}{(1+e^{-x})^2} e^{-x} \\\\\n",
    "&=& \\frac{\\partial{L}}{\\partial{y}} \\frac{1}{1+e^{-x}} \\frac{e^{-x}}{1+e^{-x}} \\\\\n",
    "&=& \\frac{\\partial{L}}{\\partial{y}} y(1-y)\n",
    "\\end{matrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식에서  \n",
    "\n",
    "1행: $y^2$ → $\\frac{1}{(1+e^{-x})^2}$  \n",
    "2행: $\\frac{1}{(1+e^{-x})^2}$ → $\\frac{1}{1+e^{-x}} \\frac{e^{-x}}{1+e^{-x}}$  \n",
    "3행: $\\frac{e^{-x}}{1+e^{-x}} = \\frac{1+e^{-x}-1}{1+e^{-x}}$ → $(1-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 sigmoid 계층의 역전파는 순전파의 출력($y$)만으로 계산 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1.0 - self.out)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Affine 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 순전파 때 수행하는 <code>행렬의 내적</code>은 기하학에서 <code>어파인 변환(affine transformation)</code>이라고 함  \n",
    "어파인 변환을 수행하는 처리를 Affine 계층이라는 이름으로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"645\" alt=\"fig 5-25\" src=\"https://user-images.githubusercontent.com/77653353/194439023-65569deb-45ca-4671-bbca-7988c8085f67.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬의 내적(dot) 노드도 곱셈 노드 처럼 서로 바꾼 값을 하류로 보냄  \n",
    "다만, 행렬의 형상에 주의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 배치용 Affine 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 하나만 고려한 것이 아닌 $N$개를 묶어서 순전파, 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"645\" alt=\"fig 5-27\" src=\"https://user-images.githubusercontent.com/77653353/194440287-883db143-a63e-4a7d-8267-ee0203973da2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"569\" alt=\"fig 5-28\" src=\"https://user-images.githubusercontent.com/77653353/194710101-77411fce-ffc4-4f62-9b0a-aa6bc0fc83a8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림과 같이 Softmax 계층은 입력 값을 정규화(출력의 합이 1이 되도록 변형)하여 출력함  \n",
    "학습(Training)시에는 위와 같이 Softmax 계층을 사용하여 예측 결과(output)를 정답(label)과 비교하여 손실(loss)을 구하지만  \n",
    "추론(Inference)시에는 Softmax 계층을 사용하지 않고, 마지막 Affine 계층의 출력을 인식 결과로 이용함  \n",
    "신경망에서 정규화하지 않는 출력 결과(위에서는 Softmax 앞의 Affine 계층의 출력)를 <code>점수(score)</code>라고 함  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax-with-Loss 계층의 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"720\" alt=\"fig 5-29\" src=\"https://user-images.githubusercontent.com/78716519/194718957-0dee0a7e-8110-42e3-8119-3b6726bf8994.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순전파와 역전파 과정은 \"부록 A. Softmax-with-Loss 계층의 계산 그래프\" 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"643\" alt=\"fig 5-30\" src=\"https://user-images.githubusercontent.com/78716519/194719029-64436a4b-729a-434c-aee8-909f22b29fc2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림은 '간소화한' Softmax-with-Loss 계층의 계산 그래프  \n",
    "여기서 주목할 것은 역전파의 결과  \n",
    "Softmax 계층의 역전파는 $(y_1-t_1,\\ y_2-t_2,\\ y_3-t_3)$라는 '말끔한' 결과 (왜 이렇게 나오는지는 부록 참고)  \n",
    "신경망의 역전파에서는 예측결과와 정답의 차이인 오차가 앞 계층에 전해지는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류 문제의 출력층에서는 '소프트맥스 함수'의 손실 함수로 '교차 엔트로피 오차'를 사용하니 역전파가 $(y_1-t_1,\\ y_2-t_2,\\ y_3-t_3)$로 말끔히 떨어짐  \n",
    "회귀 문제의 출력층에서는 '항등 함수'의 손실 함수로 '평균 제곱 오차'를 사용해도 역전파가 $(y_1-t_1,\\ y_2-t_2,\\ y_3-t_3)$로 말끔히 떨어짐  \n",
    "\n",
    "이런 말끔한 결과는 우연이 아니라 이렇게 설계했기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘못 예측한 경우  \n",
    "정답 레이블:         $(0, 1, 0)$  \n",
    "Softmax 계층의 출력: $(0.3, 0.2, 0.5)$  \n",
    "정답의 인덱스는 1이지만, 출력에서는 이때의 확률이 겨우 0.2(20%)  \n",
    "Softmax 계층의 역전파는 $(0.3, -0.8, 0.5)$라는 커다란 오차를 전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제대로 예측한 경우  \n",
    "정답 레이블:         $(0, 1, 0)$  \n",
    "Softmax 계층의 출력: $(0.01, 0.99, 0)$  \n",
    "정답의 인덱스는 1이고, 출력에서는 이때의 확률이 0.99(99%)  \n",
    "Softmax 계층의 역전파는 $(0.01, -0.01, 0)$라는 커다란 오차를 전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # 오버플로 대책\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 벡터)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파 때는 전파하는 값을 배치의 수(batch_size)로 나눠서 데이터 1개당 오차를 앞 계층으로 전파하는 점에 주의! <span style='color:pink'>!!!질문!!!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습 관련 기술들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화, 초깃값, 하이퍼파라미터, 오버피팅 대응책(가중치 감소, 드롭아웃), 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 매개변수 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것  \n",
    "이는 곧 최적 매개변수를 찾는 문제이며, 이러한 문제를 푸는 것을 `최적화(optimization)`라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 모험가 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "광대하고 복잡한 지형을 지도도 없이 눈을 가린채로 '깊은 곳'을 찾지 않으면 안되는 상황에서  \n",
    "중요한 단서가 되는 것이 땅의 `기울기`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 확률적 경사 하강법(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수식으로 쓰면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 SGD의 단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 함수의 최솟값을 구하는 문제를 생각해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x,y) = \\frac{1}{20}x^2 + y^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수의 그래프와 등고선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"648\" alt=\"fig 6-1\" src=\"https://user-images.githubusercontent.com/78716519/194982136-b6e76067-1ed4-409a-95b1-79f8f1c98424.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수의 기울기를 그려보면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"551\" alt=\"fig 6-2\" src=\"https://user-images.githubusercontent.com/78716519/194982284-f449d976-a6ca-4c88-87a3-511961e9734a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 기울기는 y축 방향은 크고, x축 방향은 작음  \n",
    "즉 y축 방향은 가파른데, x축 방향은 완만해서  \n",
    "최솟값이 되는 장소는 $(x,y) = (0,0)$이지만  \n",
    "위의 그림이 보여주는 기울기 대부분은 $(0,0)$ 방향을 가리키지 않는다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 함수에 SGD를 적용해보면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"563\" alt=\"fig 6-3\" src=\"https://user-images.githubusercontent.com/78716519/194982516-2a938087-a225-441e-b9ef-a2862bf19aa9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비효율적인 움직임 → SGD의 이러한 단점을 개선해주는 Momentum, AdaGrad, Adam 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 모멘텀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`모멘텀(momentum)`: '운동량'을 뜻하는 단어, 공이 구르는 듯한 물리 법칙에 따르는 움직임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{v} \\leftarrow \\alpha \\mathbf{v} - \\eta \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} $$\n",
    "$$ \\mathbf{W} \\leftarrow \\mathbf{W} + \\mathbf{v} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}$: 갱신할 가중치 매개변수  \n",
    "$\\frac{\\partial{L}}{\\partial{\\mathbf{W}}}$: $\\mathbf{W}$에 대한 손실함수의 기울기  \n",
    "$\\eta$: 학습률  \n",
    "$\\mathbf{v}$: 물리에서 말하는 속도(velocity)  \n",
    "$\\alpha \\mathbf{v}$항은 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할  \n",
    "$\\alpha$: 물리에서는 지면 마찰이나 공기 저항에 해당, $0.9$ 등의 값으로 설정함\n",
    "\n",
    "첫번째 식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"629\" alt=\"fig 6-4\" src=\"https://user-images.githubusercontent.com/78716519/194983373-7cfbcf3d-4f39-4667-9af5-e437fdfb93e5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None # 초기화 때는 아무 값도 담지 않음\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val) # 매개변수와 같은 구조의 데이터를 딕셔너리 변수로 저장\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모멘텀을 사용해서 6.1.3의 함수의 최적화 문제를 풀어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"557\" alt=\"fig 6-5\" src=\"https://user-images.githubusercontent.com/78716519/194984403-7a73bfa6-4e80-464e-93d9-375501962fc1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모멘텀의 갱신 경로는 공이 그릇 바닥을 구르듯 움직임  \n",
    "SGD와 비교하면 '지그재그 정도'가 덜함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서는 학습률이 중요한데  \n",
    "이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 올바른 학습을 할 수 없음  \n",
    "\n",
    "학습률을 정하는 효과적 기술로  \n",
    "`학습률 감소(learning rate decay)`: 학습을 진행하면서 학습률을 점차 줄여가는 방법, 처음에는 크게 학습하다가 조금씩 작게 학습하는 것  \n",
    "\n",
    "학습률을 서서히 낮추는 가장 간단한 방법은 매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 것  \n",
    "이를 발전시킨 것이  \n",
    "`AdaGrad`: '각각의' 매개변수에 '맞춤형' 학습률 값을 만들어줌  \n",
    "\n",
    "수식으로 쓰면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h \\leftarrow h + \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} \\odot \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} $$  \n",
    "$$ \\mathbf{W} \\leftarrow \\mathbf{W} + \\eta \\frac{1}{\\sqrt{h}} \\frac{\\partial{L}}{\\partial{\\mathbf{W}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}$: 갱신할 가중치 매개변수  \n",
    "$\\frac{\\partial{L}}{\\partial{\\mathbf{W}}}$: $\\mathbf{W}$에 대한 손실함수의 기울기  \n",
    "$\\eta$: 학습률  \n",
    "$h$: 기존 기울기 값을 제곱하여 계속 더해주는 변수 ($\\odot$기호는 행렬의 원소별 곱셈을 의미)  \n",
    "그리고 매개변수를 갱신할 때 $\\frac{1}{\\sqrt{h}}$을 곱해 학습률을 조정함  \n",
    "매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아진다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad는 과거의 기울기를 제곱하여 계속 더해감 → 학습을 진행할수록 갱신 강도가 약해짐  \n",
    "실제로 무한히 계속 학습한다면 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않게 됨  \n",
    "이 문제를 개선한 기법이  \n",
    "`RMSProp`: 과거의 모든 기울기를 균일하게 더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영함  \n",
    "이를 `지수이동평균(exponential moving average, EMA)`이라 하여, 과거 기울기의 반영 규모를 기하급수적으로 감소시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr / (np.sqrt(self.h[key]) + 1e-7) * grads[key] # 1e-7 이라는 작은 값을 더해줌으로써 분모가 0이 되지 않도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"558\" alt=\"fig 6-6\" src=\"https://user-images.githubusercontent.com/77653353/195875561-c5e9cbfe-669d-4098-b114-04023c3ced35.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Adam`: 모멘텀과 AdaGrad를 융합한 듯한 방법  \n",
    "\n",
    "Adam은 하이퍼파라미터를 3개 설정 (자세한 내용은 논문 참조 http://arxiv.org/abs/1412.6980)  \n",
    "하나는 학습률(논문에서는 $\\alpha$), 나머지 두 개는 일차 모멘텀용 계수 $\\beta_1$, 이차 모멘텀용 계수 $\\beta_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"560\" alt=\"fig 6-7\" src=\"https://user-images.githubusercontent.com/77653353/196036268-8fb9c87c-92e9-4c9c-9889-552b267e86d3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.7 어느 갱신 방법을 이용할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네 가지 기법의 결과를 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"649\" alt=\"fig 6-8\" src=\"https://user-images.githubusercontent.com/77653353/196036577-f9554cdb-f008-4c2c-9074-29dd70db4263.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 문제에서 항상 뛰어난 기법은 없지만, 주로 Adam이 많이 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 가중치의 초깃값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 초깃값을 0으로 하면?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`가중치 감소(weight decay)`: 가중치 매개변수의 값이 작아지도록 학습하는 방법, 오버피팅을 억제해 범용 성능을 높이는 테크닉  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
