{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd29463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>학습</code>: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 평균 제곱 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>평균 제곱 오차(mean squared error, MSE)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = \\frac{1}{n} \\sum_{k}(y_k - t_k)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차를 제곱하고 모두 합한 후 평균  \n",
    "\n",
    "※ 책에는 $\\frac{1}{n}$이 아니라 $\\frac{1}{2}$로 나와있는데 왜? <span style='color:pink'>!!!질문!!!</span>\n",
    "\n",
    "$y_k$: 신경망의 출력(신경망이 추정한 값)  \n",
    "$t_k$: 정답 레이블  \n",
    "$k$: 데이터의 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return np.sum((y-t)**2) / len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>교차 엔트로피 오차(cross entropy error, CEE)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = -\\sum_{k} t_{k} \\log{y_{k}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\log$는 밑이 $e$인 자연로그  \n",
    "\n",
    "$y_k$: 신경망의 출력  \n",
    "$t_k$: 정답 레이블  \n",
    "\n",
    "$t_k$는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0 (원핫인코딩)  \n",
    "→ $t_k=0$일 때는 모두 무시 가능하고, $t_k=1$일 때(정답일 때)만 자연로그 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 정답 레이블은 '2'가 정답이라 하고 이때의 신경망 출력 $y_k$가 $0.6$이라면 교차 엔트로피 오차는 $-\\log0.6 = 0.51$  \n",
    "같은 조건에서 신경망 출력 $y_k$가 $0.1$이라면 교차 엔트로피 오차는 $-\\log0.1 = 2.3$  \n",
    "(상대적으로 제대로 예측한 경우(신경망 출력이 $0.6$인 경우) loss가 작고  \n",
    " 상대적으로 잘못 예측한 경우(신경망 출력이 $0.1$인 경우) loss가 큼)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/77653353/192322965-5d57ab8b-a5b1-4b2f-a79c-c9a0cae2b55d.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 작은 값인 delta를 더하여 절대 0이 되지 않도록, 즉 마이너스 무한대가 발생하지 않도록 한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터 하나가 아닌 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 생각해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터 모두에 대한 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = - \\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log y_{nk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 $N$개라면 $t_{nk}$는 $n$번째 데이터의 $k$차원 째의 값을 의미  \n",
    "\n",
    "$y_{nk}$: 신경망의 출력  \n",
    "$t_{nk}$: 정답 레이블  \n",
    "\n",
    "수식이 좀 복잡해 보이지만 데이터 하나에 대한 손실함수를 단순히 $N$개의 데이터로 확장했을 뿐임  \n",
    "마지막에 $N$으로 나누어 정규화하여 '평균 손실 함수'를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수백만, 수천만개의 데이터를 일일이 계산하기에는 쉽지 않기 때문에  \n",
    "훈련 데이터로부터 일부만 골라 학습을 수행하는데 이 일부를 <code>미니배치(mini-batch)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:pink'>!!!질문!!!</span> ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 레이블이 원-핫 인코딩인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 레이블이 원-핫 인코딩이 아니라 '$2$'나 '$7$' 등의 숫자 레이블로 주어지는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 레이블이 원-핫 인코딩인 경우, t가 0인 원소는 교차 엔트로피 오차도 0이므로 그 계산을 무시할 수 있음  \n",
    "  \n",
    "정답 레이블이 숫자 레이블로 주어지는 경우, np.log(y[np.arange(batch_szie), t])  \n",
    "np.arange(batch_size)는 0부터 batch_size-1까지 배열을 생성함  \n",
    "(ex. batch_size=5라면, [0,1,2,3,4]라는 넘파이 배열 생성, t에는 레이블이 [2,7,0,9,4]와 같이 저장됨)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 왜 손실 함수를 설정하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 궁극적인 목적은 높은 '정확도'를 끌어내는 매개변수 값을 찾는 것!  \n",
    "그렇다면 왜 '정확도'라는 지표를 두고 손실 함수의 값을 거쳐갈까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서의 <code>미분</code>의 역할에 주목!  \n",
    "가령 가상의 신경망이 있고, 그 신경망의 어느 한 가중치 매개변수에 주목한다고 해보자  \n",
    "이때 그 가중치 매개변수의 손실 함수의 미분이란 '가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나'라는 의미  \n",
    "만약 이 미분 값이 음수면, 그 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있고  \n",
    "만약 이 미분 값이 양수면, 그 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있음  \n",
    "만약 이 미분 값이 0이면, 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 달라지지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계단 함수의 미분은 대부분의 장소(0 이외의 곳)에서 0 → 계단 함수를 활성화 함수로 이용하면 손실함수를 지표로 삼는 게 아무 의미 없어짐  \n",
    "매개변수의 작은 변화가 주는 파장을 계단 함수가 말살하여 손실 함수의 값에는 아무런 변화가 나타나지 않기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>미분</code>: 특정 순간의 변화량  \n",
    "$x$의 작은 변화가 함수$f(x)$를 얼마나 변화시키느냐를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{df(x)}{dx} = \\lim_{h→0}\\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 참고하여 함수의 미분을 구하는 계산을 구현해보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 방식은 두 가지 문제가 있는데\n",
    "- 반올림 오차 문제 → 미세한 값 $h$를 $10^{-4}$정도로 조절\n",
    "- 진정한 미분은 $x$위치의 함수의 기울기(접선)이지만 $h$를 무한히 0으로 좁히는 것 불가 → 중심 차분 또는 중앙 차분 사용($(x+h)$와 $(x-h)$일 때의 함수 $f$의 차분 계산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>수치 미분(numerical differentiation)</code>: 아주 작은 차분(임의 두 점에서의 함수 값들의 차이)으로 미분을 구하는 것  \n",
    "<code>해석적 미분(analytic differentiation)</code>: 수식을 전개해 미분을 구하는 것 ex.$y = x^2$의 미분은 $\\frac{dy}{dx} = 2x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x_0, x_1) = x_0^2 + x_1^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 함수가 있다고 해보자  \n",
    "인수들의 제곱 합을 계산하는 단순한 식이지만, 이번엔 변수가 2개인 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"473\" alt=\"fig 4-8\" src=\"https://user-images.githubusercontent.com/77653353/193848270-9e48b89b-d329-4663-9797-3095f4e597f9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>편미분</code>: 변수가 여럿인 함수에 대한 미분, $\\frac{\\partial{f}}{\\partial{x_0}}$이나 $\\frac{\\partial{f}}{\\partial{x_1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서는 $x_0$와 $x_1$의 편미분을 변수별로 따로 계산했음  \n",
    "그렇다면 $x_0$와 $x_1$의 편미분을 동시에 계산하려면?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_0=3$, $x_1=4$일 때 $(x_0, x_1)$ 양쪽의 편미분을 묶어서 $(\\frac{\\partial{f}}{\\partial{x_0}}, \\frac{\\partial{f}}{\\partial{x_1}})$처럼  \n",
    "모든 변수의 편미분을 벡터로 정리한 것을 <code>기울기(gradient)</code>라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):               # [3, 4]\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)                 # [0, 0]\n",
    "\n",
    "    for idx in range(x.size):               # idx=0, x.size=2\n",
    "        tmp_val = x[idx]                    # tmp_val = 3\n",
    "\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h                # x[0] = 3.0001\n",
    "        fxh1 = f(x)                         # fxh1 = 25.00060001\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h                # x[0] = 2.9999\n",
    "        fxh2 = f(x)                         # fxh2 = 24.99940001\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)   # grad[0] = 0.0012 / 0.0002 = 6\n",
    "        x[idx] = tmp_val # 값 복원          # x[0] = 3\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기는 함수의 '가장 낮은 장소(최솟값)'를 가리키는 것  \n",
    "기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 줄이는 방향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법(경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 목표는 학습 단계에서 최적의 매개변수를 찾는 것이고, 이는 손실 함수가 최솟값이 될 때의 매개변수 값  \n",
    "하지만 광대한 공간 속에서 어느 곳이 최솟값인지 알아내기 쉽지 않음  \n",
    "이러한 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수가 최솟값, 극솟값, 안장점이 되는 장소에서는 기울기가 0  \n",
    "- 극솟값: 국소적인 최솟값  \n",
    "- 안장점(saddle point): 어느 방향에서 보면 극댓값이고, 다른 방향에서 보면 극솟값이 되는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울어진 방향이 꼭 최솟값을 가리키는 것은 아니지만, 그 방향으로 가야 함수의 값을 줄일 수 있음  \n",
    "그래서 최솟값이 되는 장소를 찾는 문제에서는 기울기 정보를 단서로 나아갈 방향을 정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>경사법(gradient method)</code>, <code>경사 하강법(gradient descent method)</code>:  \n",
    "현 위치에서 기울어진 방향으로 일정 거리만큼 이동하고, 이동한 곳에서도 기울기를 구하고, 또 기울어진 방향으로 나아가서 함수의 값을 점차 줄이는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수식으로 나타내면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x_0 = x_0 - \\eta \\frac{\\partial{f}}{\\partial{x_0}} $$  \n",
    "$$ x_1 = x_1 - \\eta \\frac{\\partial{f}}{\\partial{x_1}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>학습률(learning rate)</code>: 매개변수 값을 얼마나 갱신하느냐를 정하는 것, 여기서는 $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 하강법 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):     # f: 최적화하려는 함수\n",
    "    x = init_x                                              # init_x: 초깃값\n",
    "                                                            # lr: 학습률\n",
    "    for i in range(step_num):                               # step_num: 경사법에 따른 반복 횟수\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>하이퍼파라미터(hyper parameter)</code>: 사람이 직접 설정해야 하는 매개변수 ex.학습률  \n",
    "<code>파라미터(parameter)</code>: 훈련 데이터와 학습 알고리즘에 의해서 '자동'으로 획득되는 매개변수 ex.가중치, 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전제  \n",
    "  신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 함. 신경망 학습은 다음과 같이 4단계로 수행함  \n",
    "  \n",
    "- 1단계 - 미니배치  \n",
    "  훈련 데이터 중 일부를 무작위로 가져옴. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것을 목표로 함  \n",
    "\n",
    "- 2단계 - 기울기 산출  \n",
    "  미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시  \n",
    "\n",
    "- 3단계 - 매개변수 갱신  \n",
    "  가중치 매개변수를 기울기 방향으로 아주 조금 갱신함  \n",
    "\n",
    "- 4단계 - 반복  \n",
    "  1~3단계를 반복함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>확률적 경사 하강법(Stochastic Gradient Descent, SGD)</code>: 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient 메서드는 수치 미분 방식으로 각 매개변수의 손실 함수에 대한 기울기를 계산  \n",
    "gradient 메서드는 오차역전파법을 사용하여 기울기를 계산 (다음 장에서 진행)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 시험 데이터로 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드에서 1에포크별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록함  \n",
    "<code>에포크(epoch)</code>: 학습에서 훈련 데이터를 모두 소진했을 때의 횟수"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('scratch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539ea8a2339509b5cf6146c54a8db679b9656bf5454007bbb0644053ad5d6fce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
