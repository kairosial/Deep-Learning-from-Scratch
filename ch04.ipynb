{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd29463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code/>학습</code>: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 평균 제곱 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code/>평균 제곱 오차(mean squared error, MSE)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = \\frac{1}{n} \\sum_{k}(y_k - t_k)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차를 제곱하고 모두 합한 후 평균  \n",
    "\n",
    "※ 책에는 $\\frac{1}{n}$이 아니라 $\\frac{1}{2}$로 나와있는데 왜? <span style='color:pink'>!!!질문!!!</span>\n",
    "\n",
    "$y_k$: 신경망의 출력(신경망이 추정한 값)  \n",
    "$t_k$: 정답 레이블  \n",
    "$k$: 데이터의 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return np.sum((y-t)**2) / len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code/>교차 엔트로피 오차(cross entropy error, CEE)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = -\\sum_{k} t_{k} \\log{y_{k}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\log$는 밑이 $e$인 자연로그  \n",
    "\n",
    "$y_k$: 신경망의 출력  \n",
    "$t_k$: 정답 레이블  \n",
    "\n",
    "$t_k$는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0 (원핫인코딩)  \n",
    "→ $t_k=0$일 때는 모두 무시 가능하고, $t_k=1$일 때(정답일 때)만 자연로그 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 정답 레이블은 '2'가 정답이라 하고 이때의 신경망 출력 $y_k$가 $0.6$이라면 교차 엔트로피 오차는 $-\\log0.6 = 0.51$  \n",
    "같은 조건에서 신경망 출력 $y_k$가 $0.1$이라면 교차 엔트로피 오차는 $-\\log0.1 = 2.3$  \n",
    "(상대적으로 제대로 예측한 경우(신경망 출력이 $0.6$인 경우) loss가 작고  \n",
    " 상대적으로 잘못 예측한 경우(신경망 출력이 $0.1$인 경우) loss가 큼)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/77653353/192322965-5d57ab8b-a5b1-4b2f-a79c-c9a0cae2b55d.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 작은 값인 delta를 더하여 절대 0이 되지 않도록, 즉 마이너스 무한대가 발생하지 않도록 한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터 하나가 아닌 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 생각해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터 모두에 대한 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = - \\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log y_{nk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 $N$개라면 $t_{nk}$는 $n$번째 데이터의 $k$차원 째의 값을 의미  \n",
    "\n",
    "$y_{nk}$: 신경망의 출력  \n",
    "$t_{nk}$: 정답 레이블  \n",
    "\n",
    "수식이 좀 복잡해 보이지만 데이터 하나에 대한 손실함수를 단순히 $N$개의 데이터로 확장했을 뿐임  \n",
    "마지막에 $N$으로 나누어 정규화하여 '평균 손실 함수'를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수백만, 수천만개의 데이터를 일일이 계산하기에는 쉽지 않기 때문에  \n",
    "훈련 데이터로부터 일부만 골라 학습을 수행하는데 이 일부를 <code/>미니배치(mini-batch)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:pink'>!!!질문!!!</span> ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('scratch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539ea8a2339509b5cf6146c54a8db679b9656bf5454007bbb0644053ad5d6fce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
